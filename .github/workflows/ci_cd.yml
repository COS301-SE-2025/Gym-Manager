name: Gym Manager CI/CD Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  lint:
    name: Lint API
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'
          cache-dependency-path: services/api/package-lock.json

      - name: Install API deps
        run: npm ci
        working-directory: ./services/api

      - name: Lint API
        run: npm run lint
        working-directory: ./services/api

  unit-tests:
    name: Run Unit Tests (API)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'
          cache-dependency-path: services/api/package-lock.json

      - name: Install API deps
        run: npm ci
        working-directory: ./services/api

      - name: Run API Unit Tests
        run: npm run test:unit
        working-directory: ./services/api
        env:
          JWT_SECRET: ci-unit-secret

  test-availability:
    name: Integration — Availability (/healthz)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'
          cache-dependency-path: services/api/package-lock.json

      - name: Install API deps
        run: npm ci
        working-directory: ./services/api

      - name: Run synthetic availability probe
        run: npm run test:integration:availability
        working-directory: ./services/api
        env:
          AVAIL_N: 60
          AVAIL_GAP_MS: 50
          AVAIL_MAX_P95_MS: 300
          AVAIL_MAX_ERR_RATE: 0
          JWT_SECRET: ci-integration-secret

  test-security:
    name: Integration — Security (JWT/Helmet/CORS)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'
          cache-dependency-path: services/api/package-lock.json

      - name: Install API deps
        run: npm ci
        working-directory: ./services/api

      - name: Run security integration tests
        run: npm run test:integration:security
        working-directory: ./services/api
        env:
          JWT_SECRET: ci-integration-secret

  test-performance:
    name: Integration — Performance (JMeter)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'
          cache-dependency-path: services/api/package-lock.json

      - name: Install API deps
        run: npm ci
        working-directory: ./services/api

      # Compile only if dist doesn't exist
      - name: Compile TypeScript (if needed)
        working-directory: ./services/api
        run: |
          set -e
          if [ -f dist/index.js ] || [ -f dist/src/index.js ]; then
            echo "dist already present — skipping compile"
          else
            # Correct npx form: install package 'typescript' temporarily and run its bin 'tsc'
            npx -y -p typescript tsc -p tsconfig.json
          fi

      - name: Start API (background) and wait for /healthz
        working-directory: ./services/api
        env:
          PORT: 3000
          NODE_ENV: test
          JWT_SECRET: ci-integration-secret
        run: |
          set -e
          # Pick a built entry file
          START=""
          for CAND in dist/index.js dist/src/index.js; do
            if [ -f "$CAND" ]; then START="$CAND"; break; fi
          done
          if [ -z "$START" ]; then echo "No built entry found in dist"; ls -la dist || true; exit 1; fi

          # Create a tiny bootstrap to require the app and listen
          cat > boot.js <<'JS'
          const path = process.env.START_FILE || 'dist/index.js';
          let mod;
          try { mod = require('./' + path); }
          catch (e) { console.error('Failed to require', path, e); process.exit(1); }
          const app = mod.app || mod.default || (mod.App ? (new mod.App()).getApp() : null);
          if (!app || !app.listen) { console.error('No Express app export (app.listen)'); process.exit(1); }
          const port = process.env.PORT || 3000;
          app.listen(port, () => console.log('API up on ' + port));
          JS

          START_FILE="$START" node boot.js &
          # Wait for healthz
          for i in {1..40}; do
            curl -fsS "http://localhost:3000/healthz" >/dev/null && break
            sleep 1
          done
          curl -fsS "http://localhost:3000/healthz" >/dev/null || (echo 'API failed to start' && exit 1)

      - name: Install JMeter
        run: |
          sudo apt-get update && sudo apt-get install -y wget unzip
          JM_VERSION=5.6.3
          wget -q https://dlcdn.apache.org/jmeter/binaries/apache-jmeter-$JM_VERSION.zip
          unzip -q apache-jmeter-$JM_VERSION.zip
          echo "$GITHUB_WORKSPACE/apache-jmeter-$JM_VERSION/bin" >> $GITHUB_PATH

      - name: Locate JMeter plan
        id: jmx
        working-directory: ./services/api
        run: |
          set -e
          if [ -f tests/perf/healthz.jmx ]; then
            PLAN="tests/perf/healthz.jmx"; OUT="tests/perf/results.jtl"
          elif [ -f src/tests/perf/healthz.jmx ]; then
            PLAN="src/tests/perf/healthz.jmx"; OUT="src/tests/perf/results.jtl"
          else
            echo "Could not find healthz.jmx in tests/perf or src/tests/perf"
            echo "Tree (services/api):"; ls -la
            echo "tests/perf:"; ls -la tests/perf 2>/dev/null || true
            echo "src/tests/perf:"; ls -la src/tests/perf 2>/dev/null || true
            exit 1
          fi
          # Fix the common typo if present
          sed -i 's/Asserion\.test_strings/Assertion.test_strings/g' "$PLAN" || true
          echo "plan=$PLAN" >> "$GITHUB_OUTPUT"
          echo "out=$OUT"   >> "$GITHUB_OUTPUT"

      - name: Run JMeter plan against local API
        working-directory: ./services/api
        env:
          JM_PLAN: ${{ steps.jmx.outputs.plan }}
          JM_OUT:  ${{ steps.jmx.outputs.out }}
          HOST: localhost
          PROTO: http
          PORT: 3000
          users: 10
          ramp: 10
          loops: 200          # NEW: drive execution via loops
          MAX_MS: 300
        run: |
          set -e
          jmeter -n \
            -t "$JM_PLAN" \
            -l "$JM_OUT" \
            -JHOST="$HOST" -JPROTO="$PROTO" -JPORT="$PORT" \
            -Jusers="$users" -Jramp="$ramp" -Jloops="$loops" -JMAX_MS="$MAX_MS"

          # Prove we actually ran requests: file exists and has samples
          test -f "$JM_OUT" || (echo "JMeter did not create results file: $JM_OUT" && exit 1)

          # Count samples (works whether XML or CSV; both include the token 'sample')
          SAMPLES=$(grep -c -i "<sample " "$JM_OUT" 2>/dev/null || true)
          if [ "$SAMPLES" -eq 0 ]; then
            # Try CSV token name if using CSV output
            SAMPLES=$(grep -c -i ",200," "$JM_OUT" 2>/dev/null || echo 0)
          fi
          echo "Sample count in results: $SAMPLES"
          if [ "${SAMPLES:-0}" -le 0 ]; then
            echo "No samples recorded — failing."
            # show tail of the log to help debug
            tail -n +1 "$JM_OUT" | head -n 50 || true
            exit 1
          fi

          # Show a small excerpt for the logs
          echo "First 10 lines of results:"
          head -n 10 "$JM_OUT" || true

      - name: Perf gates (p95 & error rate)
        working-directory: ./services/api
        env:
          JM_OUT: ${{ steps.jmx.outputs.out }}   # this is the results.jtl from your earlier step
          MAX_P95_MS: 300
          MAX_ERR_PCT: 0
        run: |
          set -e
          tail -n +2 "$JM_OUT" > /tmp/rows.csv
          N=$(wc -l < /tmp/rows.csv)
          # errors: success column is #8, responseCode is #4
          ERR=$(awk -F',' '$8=="false" || $4!="200"{c++} END{print c+0}' /tmp/rows.csv)
          ERR_PCT=$(awk -v e="$ERR" -v n="$N" 'BEGIN{ if(n==0){print 100}else{printf "%.2f", (e*100.0)/n}}')
          # p95 from elapsed column (#2)
          P95=$(cut -d, -f2 /tmp/rows.csv | sort -n | awk -v n="$N" 'BEGIN{idx=int(0.95*n); if(idx<1) idx=1} NR==idx{print; exit}')
          echo "Samples=$N, Errors=$ERR ($ERR_PCT%), p95=${P95}ms"
          # enforce thresholds
          awk -v p="$P95" -v max="$MAX_P95_MS" 'BEGIN{ if(p>max){ printf "p95 %sms > %sms\n", p, max; exit 1}}'
          awk -v e="$ERR_PCT" -v max="$MAX_ERR_PCT" 'BEGIN{ if(e>max){ printf "error rate %s%% > %s%%\n", e, max; exit 1}}'

  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [lint, unit-tests, test-availability, test-security, test-performance]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
      - uses: actions/checkout@v4
      - name: Deploy API to production
        run: echo "TODO"
      - name: Deploy UI to production
        run: echo "TODO"
